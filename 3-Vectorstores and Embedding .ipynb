{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-25T15:27:47.047653Z",
     "start_time": "2024-11-25T15:27:47.022533Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:29:37.822206Z",
     "start_time": "2024-11-25T15:29:28.324558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loaders = [\n",
    "    PyPDFLoader(\"LectureNotes/cs229-notes1.pdf\"),\n",
    "    PyPDFLoader(\"LectureNotes/cs229-notes1.pdf\"),\n",
    "    PyPDFLoader(\"LectureNotes/cs229-notes2.pdf\"),\n",
    "    PyPDFLoader(\"LectureNotes/cs229-notes3.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ],
   "id": "4f780fbff1066108",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:29:43.592093Z",
     "start_time": "2024-11-25T15:29:43.562212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ],
   "id": "674cf91658eccbfb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:29:49.024789Z",
     "start_time": "2024-11-25T15:29:49.013877Z"
    }
   },
   "cell_type": "code",
   "source": "len(splits)",
   "id": "ee5a56a18d98d532",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:30:14.314132Z",
     "start_time": "2024-11-25T15:30:13.768076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ],
   "id": "990679be4db09962",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\busra\\AppData\\Local\\Temp\\ipykernel_8156\\3400180925.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:30:24.757053Z",
     "start_time": "2024-11-25T15:30:24.753031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ],
   "id": "eed3aba4c9f788e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:30:35.785760Z",
     "start_time": "2024-11-25T15:30:32.762882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ],
   "id": "85bb71de099676a7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:31:04.387763Z",
     "start_time": "2024-11-25T15:31:04.381742Z"
    }
   },
   "cell_type": "code",
   "source": "len(embedding1)\n",
   "id": "33f82476df2313ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:31:14.352414Z",
     "start_time": "2024-11-25T15:31:14.345329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "np.dot(embedding1, embedding2)"
   ],
   "id": "b466b29359a1d9dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9631510802407719"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:31:19.960091Z",
     "start_time": "2024-11-25T15:31:19.953795Z"
    }
   },
   "cell_type": "code",
   "source": "np.dot(embedding1, embedding3)",
   "id": "a6082cad7dfd077",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7702031204123156"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:31:26.169099Z",
     "start_time": "2024-11-25T15:31:26.162553Z"
    }
   },
   "cell_type": "code",
   "source": "np.dot(embedding2, embedding3)",
   "id": "97315407d4374edf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7590539714454778"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:31:51.743249Z",
     "start_time": "2024-11-25T15:31:47.423012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'docs/chroma/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents = splits,\n",
    "    embedding = embedding,\n",
    "    persist_directory = persist_directory\n",
    ")"
   ],
   "id": "6d8f939415834fe8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:31:57.757913Z",
     "start_time": "2024-11-25T15:31:57.751828Z"
    }
   },
   "cell_type": "code",
   "source": "print(vectordb._collection.count())",
   "id": "1b29b8e0d6c3d25d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:32:07.800446Z",
     "start_time": "2024-11-25T15:32:07.793731Z"
    }
   },
   "cell_type": "code",
   "source": "question = 'Is there an email i can ask for help'",
   "id": "7c01705b13a4d6ad",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:32:15.070914Z",
     "start_time": "2024-11-25T15:32:14.182822Z"
    }
   },
   "cell_type": "code",
   "source": "docs = vectordb.similarity_search(question, k=3)",
   "id": "36e89a4b328e7e81",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:32:22.440359Z",
     "start_time": "2024-11-25T15:32:22.432742Z"
    }
   },
   "cell_type": "code",
   "source": "len(docs)",
   "id": "50748d985df1b141",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:32:27.504488Z",
     "start_time": "2024-11-25T15:32:27.495773Z"
    }
   },
   "cell_type": "code",
   "source": "docs[0].page_content",
   "id": "e7e95856105d52bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'diﬀerent documents. For instance, if an email starts with “A NIPS . . . ,”\\nthen x1 = 1 (“a” is the ﬁrst word in the dictionary), and x2 = 35000 (if\\n“nips” is the 35000th word in the dictionary).\\nIn the multinomial event model, we assume that the way an email is\\ngenerated is via a random process in which spam/non-spam is ﬁrst de ter-\\nmined (according to p(y)) as before. Then, the sender of the email writes the\\nemail by ﬁrst generating x1 from some multinomial distribution over words\\n(p(x1|y)). Next, the second word x2 is chosen independently of x1 but from\\nthe same multinomial distribution, and similarly for x3, x4, and so on, until\\nall n words of the email have been generated. Thus, the overall proba bility of\\na message is given by p(y) ∏ n\\ni=1 p(xi|y). Note that this formula looks like the\\none we had earlier for the probability of a message under the multi-va riate\\nBernoulli event model, but that the terms in the formula now mean ve ry dif-\\nferent things. In particular xi|y is now a multinomial, rather than a Bernoulli\\ndistribution.\\nThe parameters for our new model are φy = p(y) as before, φk|y=1 =\\np(xj = k|y = 1) (for any j) and φi|y=0 = p(xj = k|y = 0). Note that we have\\nassumed that p(xj |y) is the same for all values of j (i.e., that the distribution\\naccording to which a word is generated does not depend on its positio n j\\nwithin the email).'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:32:39.854374Z",
     "start_time": "2024-11-25T15:32:39.846106Z"
    }
   },
   "cell_type": "code",
   "source": "question = \"what did they say about matlab?\"",
   "id": "6d0cecb966d045a7",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:33:02.052914Z",
     "start_time": "2024-11-25T15:33:00.232690Z"
    }
   },
   "cell_type": "code",
   "source": "docs = vectordb.similarity_search(question,k=5)",
   "id": "21437e6050e44384",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:33:24.675127Z",
     "start_time": "2024-11-25T15:33:24.662841Z"
    }
   },
   "cell_type": "code",
   "source": "docs[0]",
   "id": "f304ba9e83f14dad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'page': 14, 'source': 'LectureNotes/cs229-notes1.pdf'}, page_content='that we saw earlier is known as a parametric learning algorithm, because\\nit has a ﬁxed, ﬁnite number of parameters (the θi’s), which are ﬁt to the\\ndata. Once we’ve ﬁt the θi’s and stored them away, we no longer need to\\nkeep the training data around to make future predictions. In cont rast, to\\nmake predictions using locally weighted linear regression, we need to k eep\\nthe entire training set around. The term “non-parametric” (roug hly) refers\\nto the fact that the amount of stuﬀ we need to keep in order to rep resent the\\nhypothesis h grows linearly with the size of the training set.\\n4If x is vector-valued, this is generalized to be w(i) = exp( −(x(i) − x)T (x(i) − x)/ (2τ2)),\\nor w(i) = exp( −(x(i) − x)T Σ −1(x(i) − x)/ 2), for an appropriate choice of τ or Σ.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:33:44.410559Z",
     "start_time": "2024-11-25T15:33:44.402859Z"
    }
   },
   "cell_type": "code",
   "source": "question = \"what did they say about regression in the third lecture?\"",
   "id": "4e6c0088a75d0624",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:33:50.816368Z",
     "start_time": "2024-11-25T15:33:50.343851Z"
    }
   },
   "cell_type": "code",
   "source": "docs = vectordb.similarity_search(question,k=5)",
   "id": "6283bb1079ccf756",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:33:55.929101Z",
     "start_time": "2024-11-25T15:33:55.922740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ],
   "id": "5218c472787e1f49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 2, 'source': 'LectureNotes/cs229-notes1.pdf'}\n",
      "{'page': 2, 'source': 'LectureNotes/cs229-notes1.pdf'}\n",
      "{'page': 2, 'source': 'LectureNotes/cs229-notes1.pdf'}\n",
      "{'page': 2, 'source': 'LectureNotes/cs229-notes1.pdf'}\n",
      "{'page': 18, 'source': 'LectureNotes/cs229-notes1.pdf'}\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:34:04.239447Z",
     "start_time": "2024-11-25T15:34:04.232670Z"
    }
   },
   "cell_type": "code",
   "source": "print(docs[4].page_content)",
   "id": "c621d01ab8055537",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the 1960s, this “perceptron” was argued to be a rough model f or how\n",
      "individual neurons in the brain work. Given how simple the algorithm is, it\n",
      "will also provide a starting point for our analysis when we talk about learning\n",
      "theory later in this class. Note however that even though the perc eptron may\n",
      "be cosmetically similar to the other algorithms we talked about, it is act ually\n",
      "a very diﬀerent type of algorithm than logistic regression and least s quares\n",
      "linear regression; in particular, it is diﬃcult to endow the perceptron ’s predic-\n",
      "tions with meaningful probabilistic interpretations, or derive the pe rceptron\n",
      "as a maximum likelihood estimation algorithm.\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:34:11.488908Z",
     "start_time": "2024-11-25T15:34:11.482844Z"
    }
   },
   "cell_type": "code",
   "source": "print(docs[2].page_content)",
   "id": "d9ee80ca3687fa0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Part I\n",
      "Linear Regression\n",
      "To make our housing example more interesting, let’s consider a slightlyricher\n",
      "dataset in which we also know the number of bedrooms in each house:\n",
      "Living area (feet 2)\n",
      "#bedrooms Price (1000$s)\n",
      "2104 3 400\n",
      "1600 3 330\n",
      "2400 3 369\n",
      "1416 2 232\n",
      "3000 4 540\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "Here, the x’s are two-dimensional vectors in R2. For instance, x(i)\n",
      "1 is the\n",
      "living area of the i-th house in the training set, and x(i)\n",
      "2 is its number of\n",
      "bedrooms. (In general, when designing a learning problem, it will be up to\n",
      "you to decide what features to choose, so if you are out in Portland gathering\n",
      "housing data, you might also decide to include other features such a s whether\n",
      "each house has a ﬁreplace, the number of bathrooms, and so on. W e’ll say\n",
      "more about feature selection later, but for now let’s take the feat ures as\n",
      "given.)\n",
      "To perform supervised learning, we must decide how we’re going to re p-\n",
      "resent functions/hypotheses h in a computer. As an initial choice, let’s say\n",
      "we decide to approximate y as a linear function of x:\n",
      "hθ(x) = θ0 + θ1x1 + θ2x2\n",
      "Here, the θi’s are the parameters (also called weights) parameterizing the\n",
      "space of linear functions mapping from X to Y. When there is no risk of\n",
      "confusion, we will drop the θ subscript in hθ(x), and write it more simply as\n",
      "h(x). To simplify our notation, we also introduce the convention of lettin g\n",
      "x0 = 1 (this is the intercept term ), so that\n",
      "h(x) =\n",
      "n∑\n",
      "i=0\n",
      "θixi = θT x,\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
