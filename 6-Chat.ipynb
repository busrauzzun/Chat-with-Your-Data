{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-24T11:40:19.490745Z",
     "start_time": "2024-11-24T11:40:19.475618Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:44:09.898701Z",
     "start_time": "2024-11-24T11:44:04.892946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "persist_directory = 'docs/chroma'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ],
   "id": "cc81e77021036993",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\busra\\AppData\\Local\\Temp\\ipykernel_18932\\881590357.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:08:22.674294Z",
     "start_time": "2024-11-24T12:08:21.472303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = 'What are major topics for this class?'\n",
    "docs = vectordb.similarity_search(question, k=3)\n",
    "len(docs)"
   ],
   "id": "83839e9e90489a6f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:10:18.836888Z",
     "start_time": "2024-11-24T12:10:17.645990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "llm.invoke('Hello world!')"
   ],
   "id": "a1eebdb23fda43a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 10, 'total_tokens': 19, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-eb051681-74ff-42d8-910a-321f2cb91aae-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:12:39.396144Z",
     "start_time": "2024-11-24T12:12:37.459912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "question = \"Is probability a class topic?\"\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "result[\"result\"]"
   ],
   "id": "fa3c38693112274",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, probability is a class topic. Thanks for asking!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:15:13.077243Z",
     "start_time": "2024-11-24T12:15:13.070037Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "685e114128a83032",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Is probability a class topic?',\n",
       " 'result': 'Yes, probability is a class topic. Thanks for asking!',\n",
       " 'source_documents': [Document(metadata={'page': 12, 'source': 'LectureNotes/cs229-notes2.pdf'}, page_content='13\\n2.2 Event models for text classiﬁcation\\nTo close oﬀ our discussion of generative learning algorithms, let’s talkabout\\none more model that is speciﬁcally for text classiﬁcation. While Naive B ayes\\nas we’ve presented it will work well for many classiﬁcation problems, f or text\\nclassiﬁcation, there is a related model that does even better.\\nIn the speciﬁc context of text classiﬁcation, Naive Bayes as prese nted uses\\nthe what’s called the multi-variate Bernoulli event model . In this model,\\nwe assumed that the way an email is generated is that ﬁrst it is rando mly\\ndetermined (according to the class priors p(y)) whether a spammer or non-\\nspammer will send you your next message. Then, the person sendin g the\\nemail runs through the dictionary, deciding whether to include each word i\\nin that email independently and according to the probabilities p(xi = 1 |y) =\\nφi|y. Thus, the probability of a message was given by p(y) ∏ n\\ni=1 p(xi|y).\\nHere’s a diﬀerent model, called the multinomial event model . To de-\\nscribe this model, we will use a diﬀerent notation and set of features for\\nrepresenting emails. We let xi denote the identity of the i-th word in the\\nemail. Thus, xi is now an integer taking values in {1, . . . , |V |}, where |V |\\nis the size of our vocabulary (dictionary). An email of n words is now rep-\\nresented by a vector ( x1, x2, . . . , x n) of length n; note that n can vary for\\ndiﬀerent documents. For instance, if an email starts with “A NIPS . . . ,”'),\n",
       "  Document(metadata={'page': 15, 'source': 'LectureNotes/cs229-notes1.pdf'}, page_content='16\\nPart II\\nClassiﬁcation and logistic\\nregression\\nLet’s now talk about the classiﬁcation problem. This is just like the regression\\nproblem, except that the values y we now want to predict take on only\\na small number of discrete values. For now, we will focus on the binary\\nclassiﬁcation problem in which y can take on only two values, 0 and 1.\\n(Most of what we say here will also generalize to the multiple-class cas e.)\\nFor instance, if we are trying to build a spam classiﬁer for email, then x(i)\\nmay be some features of a piece of email, and y may be 1 if it is a piece\\nof spam mail, and 0 otherwise. 0 is also called the negative class , and 1\\nthe positive class , and they are sometimes also denoted by the symbols “-”\\nand “+.” Given x(i), the corresponding y(i) is also called the label for the\\ntraining example.\\n5 Logistic regression\\nWe could approach the classiﬁcation problem ignoring the fact thaty is\\ndiscrete-valued, and use our old linear regression algorithm to try t o predict\\ny given x. However, it is easy to construct examples where this method\\nperforms very poorly. Intuitively, it also doesn’t make sense for hθ(x) to take\\nvalues larger than 1 or smaller than 0 when we know that y ∈ { 0, 1}.\\nTo ﬁx this, let’s change the form for our hypotheses hθ(x). We will choose\\nhθ(x) = g(θT x) = 1\\n1 + e−θT x ,\\nwhere\\ng(z) = 1\\n1 + e−z\\nis called the logistic function or the sigmoid function . Here is a plot\\nshowing g(z):'),\n",
       "  Document(metadata={'page': 15, 'source': 'LectureNotes/cs229-notes1.pdf'}, page_content='16\\nPart II\\nClassiﬁcation and logistic\\nregression\\nLet’s now talk about the classiﬁcation problem. This is just like the regression\\nproblem, except that the values y we now want to predict take on only\\na small number of discrete values. For now, we will focus on the binary\\nclassiﬁcation problem in which y can take on only two values, 0 and 1.\\n(Most of what we say here will also generalize to the multiple-class cas e.)\\nFor instance, if we are trying to build a spam classiﬁer for email, then x(i)\\nmay be some features of a piece of email, and y may be 1 if it is a piece\\nof spam mail, and 0 otherwise. 0 is also called the negative class , and 1\\nthe positive class , and they are sometimes also denoted by the symbols “-”\\nand “+.” Given x(i), the corresponding y(i) is also called the label for the\\ntraining example.\\n5 Logistic regression\\nWe could approach the classiﬁcation problem ignoring the fact thaty is\\ndiscrete-valued, and use our old linear regression algorithm to try t o predict\\ny given x. However, it is easy to construct examples where this method\\nperforms very poorly. Intuitively, it also doesn’t make sense for hθ(x) to take\\nvalues larger than 1 or smaller than 0 when we know that y ∈ { 0, 1}.\\nTo ﬁx this, let’s change the form for our hypotheses hθ(x). We will choose\\nhθ(x) = g(θT x) = 1\\n1 + e−θT x ,\\nwhere\\ng(z) = 1\\n1 + e−z\\nis called the logistic function or the sigmoid function . Here is a plot\\nshowing g(z):'),\n",
       "  Document(metadata={'page': 10, 'source': 'LectureNotes/cs229-notes2.pdf'}, page_content='11\\nThus, for a house with living area 890 square feet, we would set the v alue\\nof the corresponding feature xi to 3. We can then apply the Naive Bayes\\nalgorithm, and model p(xi|y) with a multinomial distribution, as described\\npreviously. When the original, continuous-valued attributes are no t well-\\nmodeled by a multivariate normal distribution, discretizing the featu res and\\nusing Naive Bayes (instead of GDA) will often result in a better classiﬁ er.\\n2.1 Laplace smoothing\\nThe Naive Bayes algorithm as we have described it will work fairly well\\nfor many problems, but there is a simple change that makes it work much\\nbetter, especially for text classiﬁcation. Let’s brieﬂy discuss a pro blem with\\nthe algorithm in its current form, and then talk about how we can ﬁx it .\\nConsider spam/email classiﬁcation, and let’s suppose that, after c omplet-\\ning CS229 and having done excellent work on the project, you decide around\\nJune 2003 to submit the work you did to the NIPS conference for pu blication.\\n(NIPS is one of the top machine learning conferences, and the dead line for\\nsubmitting a paper is typically in late June or early July.) Because you en d\\nup discussing the conference in your emails, you also start getting m essages\\nwith the word “nips” in it. But this is your ﬁrst NIPS paper, and until t his\\ntime, you had not previously seen any emails containing the word “nips ”;\\nin particular “nips” did not ever appear in your training set of spam/n on-')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:17:12.165404Z",
     "start_time": "2024-11-24T12:17:12.101974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True\n",
    ")"
   ],
   "id": "8e5a32577bd1c601",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\busra\\AppData\\Local\\Temp\\ipykernel_18932\\303388722.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:17:29.535315Z",
     "start_time": "2024-11-24T12:17:29.506377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ],
   "id": "d2b11f5e4bf8dc0e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:17:41.946446Z",
     "start_time": "2024-11-24T12:17:39.950378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"Is probability a class topic?\"\n",
    "result = qa.invoke({\"question\": question})"
   ],
   "id": "18505b5b335aad5d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:18:01.491411Z",
     "start_time": "2024-11-24T12:18:01.485191Z"
    }
   },
   "cell_type": "code",
   "source": "result['answer']",
   "id": "8ebca98cb3ae13c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, probability is a class topic. It is often covered in classes related to statistics, machine learning, data science, and mathematics. Probability theory is fundamental in understanding uncertainty and making predictions based on data.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:18:15.504197Z",
     "start_time": "2024-11-24T12:18:13.186411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"why are those prerequesites needed?\"\n",
    "result = qa.invoke({\"question\": question})"
   ],
   "id": "f4392dd12178d055",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:18:19.689831Z",
     "start_time": "2024-11-24T12:18:19.685520Z"
    }
   },
   "cell_type": "code",
   "source": "result['answer']",
   "id": "664662b541900feb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have enough information to answer that question based on the context provided.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:56:30.171882Z",
     "start_time": "2024-11-24T12:56:30.166261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ],
   "id": "63a560e3bb0b1d75",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:58:22.830795Z",
     "start_time": "2024-11-24T12:58:22.813801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_db(file, chain_type, k):\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    retriever = db.as_retriever(search_type='similarity', search_kwargs={\"k\": k})\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0),\n",
    "        chain_type=chain_type,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        #return_generated_questions=True,\n",
    "    )\n",
    "    return qa"
   ],
   "id": "fe562226292108cb",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:58:23.165369Z",
     "start_time": "2024-11-24T12:58:23.142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    chat_history = param.List([])\n",
    "    answer = param.String(\"\")\n",
    "    db_query  = param.String(\"\")\n",
    "    db_response = param.List([])\n",
    "    \n",
    "    def __init__(self,  **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.loaded_file = \"LectureNotes/cs229-notes1.pdf\"\n",
    "        self.qa = load_db(self.loaded_file,\"stuff\", 4)\n",
    "    \n",
    "    def call_load_db(self, count):\n",
    "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
    "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "        else:\n",
    "            file_input.save(\"temp.pdf\")  # local copy\n",
    "            self.loaded_file = file_input.filename\n",
    "            button_load.button_style=\"outline\"\n",
    "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
    "            button_load.button_style=\"solid\"\n",
    "        self.clr_history()\n",
    "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "\n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.extend([(query, result[\"answer\"])])\n",
    "        self.db_query = result[\"generated_question\"]\n",
    "        self.db_response = result[\"source_documents\"]\n",
    "        self.answer = result['answer'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        inp.value = ''  #clears loading indicator when cleared\n",
    "        return pn.WidgetBox(*self.panels,scroll=True)\n",
    "\n",
    "    @param.depends('db_query ', )\n",
    "    def get_lquest(self):\n",
    "        if not self.db_query :\n",
    "            return pn.Column(\n",
    "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
    "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
    "            )\n",
    "        return pn.Column(\n",
    "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
    "            pn.pane.Str(self.db_query )\n",
    "        )\n",
    "\n",
    "    @param.depends('db_response', )\n",
    "    def get_sources(self):\n",
    "        if not self.db_response:\n",
    "            return \n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for doc in self.db_response:\n",
    "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    @param.depends('convchain', 'clr_history') \n",
    "    def get_chats(self):\n",
    "        if not self.chat_history:\n",
    "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for exchange in self.chat_history:\n",
    "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return "
   ],
   "id": "9f7c370c830040a",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:58:29.323299Z",
     "start_time": "2024-11-24T12:58:23.457272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cb = cbfs()\n",
    "\n",
    "file_input = pn.widgets.FileInput(accept='.pdf')\n",
    "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
    "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
    "button_clearhistory.on_click(cb.clr_history)\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
    "\n",
    "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "jpg_pane = pn.pane.Image( './img/convchain.jpg')\n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab2= pn.Column(\n",
    "    pn.panel(cb.get_lquest),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(cb.get_sources ),\n",
    ")\n",
    "tab3= pn.Column(\n",
    "    pn.panel(cb.get_chats),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab4=pn.Column(\n",
    "    pn.Row( file_input, button_load, bound_button_load),\n",
    "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
    "    pn.layout.Divider(),\n",
    "    pn.Row(jpg_pane.clone(width=400))\n",
    ")\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
    "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
    ")\n",
    "dashboard"
   ],
   "id": "311ae824d73dba7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:param.Column00193: Displaying Panel objects in the notebook requires the panel extension to be loaded. Ensure you run pn.extension() before displaying objects in the notebook.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Column\n",
       "    [0] Row\n",
       "        [0] Markdown(str)\n",
       "    [1] Tabs\n",
       "        [0] Column\n",
       "            [0] Row\n",
       "                [0] TextInput(placeholder='Enter text here…')\n",
       "            [1] Divider()\n",
       "            [2] ParamFunction(function, _pane=WidgetBox, defer_load=False, height=300, loading_indicator=True)\n",
       "            [3] Divider()\n",
       "        [1] Column\n",
       "            [0] ParamMethod(method, _pane=Column, defer_load=False)\n",
       "            [1] Divider()\n",
       "            [2] ParamMethod(method, _pane=Str, defer_load=False)\n",
       "        [2] Column\n",
       "            [0] ParamMethod(method, _pane=WidgetBox, defer_load=False)\n",
       "            [1] Divider()\n",
       "        [3] Column\n",
       "            [0] Row\n",
       "                [0] FileInput(accept='.pdf')\n",
       "                [1] Button(button_type='primary', name='Load DB')\n",
       "                [2] ParamFunction(function, _pane=Markdown, defer_load=False)\n",
       "            [1] Row\n",
       "                [0] Button(button_type='warning', name='Clear History')\n",
       "                [1] Markdown(str)\n",
       "            [2] Divider()\n",
       "            [3] Row\n",
       "                [0] Image(str, width=400)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
